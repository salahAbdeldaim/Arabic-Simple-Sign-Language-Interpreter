{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d32b43",
   "metadata": {},
   "source": [
    "# 1- Imports and Warning Suppression Setup\n",
    "\n",
    "\n",
    "Explanation:\n",
    "Imports the warnings module and disables Python warnings so that no warning messages appear in the output during execution.\n",
    "\n",
    "If removed:\n",
    "You may see warnings from libraries (like deprecation warnings) that could clutter the terminal, but the code will usually still run normally.\n",
    "\n",
    "\n",
    "**Explanation for each library**:\n",
    "\n",
    "- `cv2`: OpenCV, used for video processing, drawing on frames, and displaying windows.\n",
    "\n",
    "- `mediapipe`: A ready-made library for hand tracking (landmarks).\n",
    "\n",
    "- `numpy`: For array operations and handling images as arrays.\n",
    "\n",
    "- `joblib`: For loading the saved model (pickled model).\n",
    "\n",
    "- `arabic_reshaper + bidi.algorithm.get_display`: Reconstructs Arabic characters and fixes direction for correct display (used to render Arabic text with PIL/OpenCV).\n",
    "\n",
    "- `PIL` (ImageFont, ImageDraw, Image): Used to draw Arabic text on images because OpenCV does not support Arabic rendering properly.\n",
    "\n",
    "**If any of these are removed**:\n",
    "The part of the code depending on that library will fail (e.g., without joblib you can't load the model; without arabic_reshaper, Arabic text will appear disjointed).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e6480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import joblib\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from PIL import ImageFont, ImageDraw, Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205b693",
   "metadata": {},
   "source": [
    "## 2-  General Settings / Constant Variables\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- `MODEL_PATH`: Path to the saved model file (pickle).\n",
    "\n",
    "- `THRESHOLD_FRAMES`: Number of frames required to confirm a predicted letter before adding it â€” prevents capturing multiple times from a single frame.\n",
    "\n",
    "- `SPACE_COOLDOWN_MAX`: Number of cooldown frames after adding a space (SPACE) before allowing another one.\n",
    "\n",
    "- `myCamere`: Camera index (0 is usually the default webcam).\n",
    "\n",
    "# UI settings:\n",
    "Top/bottom bar height and minimum window width (to avoid overlap in UI elements).\n",
    "\n",
    "**If changed/removed**:\n",
    "\n",
    "- Lowering `THRESHOLD_FRAMES` â†’ faster but more prone to errors.\n",
    "\n",
    "- Increasing it â†’ slower input but more stable.\n",
    "\n",
    "- Setting `MIN_WINDOW_WIDTH` too small â†’ text may overlap or collide with UI elements.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n",
    "# =============================================================\n",
    "MODEL_PATH = r\"linearSVC.pkl\"\n",
    "THRESHOLD_FRAMES = 25   \n",
    "SPACE_COOLDOWN_MAX = 40 \n",
    "myCamere = 0\n",
    "\n",
    "# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù€ UI (Ø¨Ø§Ù„Ø¨ÙƒØ³Ù„)\n",
    "UI_TOP_HEIGHT = 50      # Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¹Ù„ÙˆÙŠ\n",
    "UI_BOTTOM_HEIGHT = 80  # Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø³ÙÙ„ÙŠ\n",
    "MIN_WINDOW_WIDTH = 800 # ðŸ‘ˆ Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ø«Ø§Ø¨Øª (Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ad68a",
   "metadata": {},
   "source": [
    "## 3- State Variables\n",
    "\n",
    "\n",
    "**Explanation**: Variables that store the current state:\n",
    "\n",
    "- `sentence`: The actual constructed text.\n",
    "\n",
    "- `last_prediction`: The last predicted letter â€” used to check stability across frames.\n",
    "\n",
    "- `frame_counter`: Counts how many consecutive frames kept the same prediction.\n",
    "\n",
    "- `space_cooldown`: Cooldown counter for adding a SPACE.\n",
    "\n",
    "- `writing_enabled`: Whether writing mode is enabled or not.\n",
    "\n",
    "**If removed**:\n",
    "Text tracking and input stabilization would break or behave incorrectly.\n",
    "\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…\n",
    "sentence = \"\"\n",
    "last_prediction = None\n",
    "frame_counter = 0\n",
    "space_cooldown = 0\n",
    "writing_enabled = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde5eb4",
   "metadata": {},
   "source": [
    "## 4- Loading the Model\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Attempts to load the file `linearSVC.pkl`.\n",
    "\n",
    "- Some people store models in a dictionary like `{'model': model, ...}`, so this code handles both possibilities.\n",
    "\n",
    "- If loading fails, it prints an error and exits the program (`exit()`).\n",
    "\n",
    "**If `try/except` is removed**:\n",
    "A loading error would raise an exception and crash the program in a less friendly way.\n",
    "\n",
    "**Note**:\n",
    "During development, you may prefer printing the actual error cause:\n",
    "`except Exception as e: print(e)`\n",
    "\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â³ Loading Model...\")\n",
    "try:\n",
    "    loaded_data = joblib.load(MODEL_PATH)\n",
    "    if isinstance(loaded_data, dict) and 'model' in loaded_data:\n",
    "        model = loaded_data['model']\n",
    "    else:\n",
    "        model = loaded_data\n",
    "    print(\"âœ… Model Loaded.\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error loading model.\")\n",
    "    print(e)\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b577b",
   "metadata": {},
   "source": [
    "## 5- Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2174e67",
   "metadata": {},
   "source": [
    "`normalize_char`\n",
    "\n",
    "**Explanation**:\n",
    "Normalizes certain Arabic letters into a unified form `(here it converts \"Ø£\" to \"Ø§\")`.\n",
    "\n",
    "**If removed**:\n",
    "Visual variations may appear (e.g., `\"Ø£\"` and `\"Ø§\"` would be treated as different characters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char(char):\n",
    "    if char == 'Ø£': return 'Ø§'\n",
    "    return char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c737176",
   "metadata": {},
   "source": [
    "\n",
    "`extract_features`\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "Converts Mediapipe `hand_landmarks` coordinates into a feature list relative to a base point (index 0).\n",
    "\n",
    "- Assumes `points[0]` is the reference point (usually the wrist).\n",
    "\n",
    "- Returns coordinate differences for each landmark â€” producing a representation independent of camera distance/position.\n",
    "\n",
    "**If removed/changed**:\n",
    "The model expects features in a specific format.\n",
    "Changing the extraction method will break compatibility with the trained model.\n",
    "Using raw coordinates without normalization would make the model sensitive to distance and hand position.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389a95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(hand_landmarks):\n",
    "    points = []\n",
    "    for lm in hand_landmarks.landmark:\n",
    "        points.append([lm.x, lm.y, lm.z])\n",
    "    base_x, base_y, base_z = points[0]\n",
    "    final_features = []\n",
    "    for i in range(1, len(points)):\n",
    "        p = points[i]\n",
    "        final_features.extend([p[0] - base_x, p[1] - base_y, p[2] - base_z])\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccad3fb",
   "metadata": {},
   "source": [
    "`draw_arabic_text`\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "Draws Arabic text on an image (numpy array) using PIL:\n",
    "\n",
    "- Converts the numpy image to a PIL image.\n",
    "\n",
    "- Uses `arabic_reshaper` + `bidi.get_display` to render Arabic letters correctly.\n",
    "\n",
    "Draws the text using the chosen font, falling back to default if `arial.ttf` is missing.\n",
    "\n",
    "- Returns the modified image as a numpy array.\n",
    "\n",
    "**If removed**:\n",
    "Displaying Arabic text using `cv2.putText` results in broken or reversed letters.\n",
    "Arabic shaping and direction handling will not work.\n",
    "This function is essential for proper Arabic rendering.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c373257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arabic_text(img, text, position, color=(0, 255, 0), font_size=32):\n",
    "    img_pil = Image.fromarray(img)\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    position = (int(position[0]), int(position[1]))\n",
    "    draw.text(position, bidi_text, font=font, fill=color)\n",
    "    return np.array(img_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd968d0",
   "metadata": {},
   "source": [
    "## 6- Mediapipe Setup\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- `mp_hands` and `mp_drawing` provide hand-tracking and drawing utilities.\n",
    "\n",
    "- `Hands` is configured for:\n",
    "\n",
    "- Video input (not static images),\n",
    "\n",
    "- Up to 2 hands,\n",
    "\n",
    "- Confidence thresholds for detection and tracking.\n",
    "\n",
    "**If values are changed**:\n",
    "\n",
    "- Lower `min_detection_confidence` â†’ more false positives.\n",
    "\n",
    "- Higher values â†’ may fail to detect hands in poor lighting.\n",
    "\n",
    "- `max_num_hands` > 2 is unnecessary in most cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2e914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Ø¥Ø¹Ø¯Ø§Ø¯ Mediapipe\n",
    "# =============================================================\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7988028",
   "metadata": {},
   "source": [
    "## 7- Camera Initialization & Resolution Setup\n",
    "```py\n",
    "cap = cv2.VideoCapture(myCamere)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Opens your device camera (using index `myCamere`) and attempts to set the resolution to 1280Ã—720.\n",
    "\n",
    "If `cap.set` is removed:\n",
    "The camera will run at its default resolution (usually lower).\n",
    "Note: Some cameras cannot support the requested resolution and will fallback to what they support.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## 8- The Main Loop\n",
    "```py\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "A loop that keeps capturing frames as long as the camera is open.\n",
    "\n",
    "If `ret` is `False`, this means a capture error or end of stream â†’ break.\n",
    "\n",
    "```py\n",
    "h_cam, w_cam, _ = frame.shape\n",
    "rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(rgb_frame)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Retrieves the frame dimensions.\n",
    "\n",
    "- Converts BGR â†’ RGB because Mediapipe expects RGB.\n",
    "\n",
    "- `hands.process` returns the detected hand landmarks.\n",
    "\n",
    "**If RGB conversion is forgotten**:\n",
    "Mediapipe may perform poorly or give incorrect landmark detection because it would receive incorrectly ordered color channels.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 9- Hand Processing (If results are found)\n",
    "```py\n",
    "if results.multi_hand_landmarks:\n",
    "    hands_count = len(results.multi_hand_landmarks)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "If one or more hands are detected, process them depending on how many hands are found.\n",
    "\n",
    "---\n",
    "\n",
    "**Case: Two Hands Detected (`hands_count == 2`)**\n",
    "```py\n",
    "if hands_count == 2:\n",
    "    frame_counter = 0\n",
    "    if writing_enabled and space_cooldown == 0:\n",
    "        sentence += \" \"\n",
    "        space_cooldown = SPACE_COOLDOWN_MAX\n",
    "\n",
    "    cv2.putText(frame, \"SPACE\", (50, h_cam - 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "Two hands are treated as the gesture for *SPACE*. When this occurs:\n",
    "\n",
    "- Reset `frame_counter` (to avoid recording a letter at this moment).\n",
    "\n",
    "- If writing is enabled and there is no cooldown, add a space to `sentence` and start the cooldown timer.\n",
    "\n",
    "- Display `\"SPACE\"` on the frame.\n",
    "\n",
    "- Draw landmarks for both hands.\n",
    "\n",
    "**If removed**:\n",
    "The system will not detect the SPACE gesture (2-hand gesture), will not insert spaces, and will not display `\"SPACE\"`.\n",
    "\n",
    "---\n",
    "**Case: One Hand Detected (`hands_count == 1`)**\n",
    "```py\n",
    "elif hands_count == 1:\n",
    "    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    data = extract_features(hand_landmarks)\n",
    "    raw_char = \"?\"\n",
    "    try:\n",
    "        prediction = model.predict([data])[0]\n",
    "        raw_char = str(prediction)\n",
    "    except: pass\n",
    "\n",
    "    current_char = normalize_char(raw_char)\n",
    "```\n",
    "\n",
    "---\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- Obtain the single hand's landmarks and draw them.\n",
    "\n",
    "- Extract features using `extract_features`.\n",
    "\n",
    "- Feed the features to the model to predict the corresponding character.\n",
    "\n",
    "- If prediction fails (incorrect data shape, etc.), ignore and keep `\"?\"`.\n",
    "\n",
    "- Normalize Arabic characters using `normalize_char`.\n",
    "\n",
    "**If the try/except is removed**:\n",
    "If the model throws any error, the entire loop would crash.\n",
    "The `try` ensures the app keeps running even when a prediction occasionally fails.\n",
    "\n",
    "---\n",
    "\n",
    "**Frame Stability Logic (Debouncing)**\n",
    "```py\n",
    "if current_char == last_prediction:\n",
    "    frame_counter += 1\n",
    "else:\n",
    "    frame_counter = 0\n",
    "    last_prediction = current_char\n",
    "\n",
    "if frame_counter >= THRESHOLD_FRAMES:\n",
    "    if writing_enabled:\n",
    "        sentence += current_char\n",
    "        frame_counter = 0\n",
    "    else:\n",
    "        frame_counter = THRESHOLD_FRAMES\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Compares the current prediction with the last one.\n",
    "\n",
    "- If the same prediction persists for `THRESHOLD_FRAMES`, it is considered stable.\n",
    "\n",
    "- Only then is the character added to `sentence` (if writing is enabled).\n",
    "\n",
    "This avoids recording letters from noisy or unstable frames.\n",
    "\n",
    "**If removed**:\n",
    "The system would add a letter immediately on every prediction â†’ extremely noisy, unstable, and full of repeated letters.\n",
    "\n",
    "---\n",
    "\n",
    "**Using Hand Position for Drawing (Progress Bar & Character Box)**\n",
    "```py\n",
    "cx, cy = int(hand_landmarks.landmark[0].x * w_cam), int(hand_landmarks.landmark[0].y * h_cam)\n",
    "bar_width = int((frame_counter / THRESHOLD_FRAMES) * 100)\n",
    "bar_color = (0, 255, 0) if writing_enabled else (0, 200, 255)\n",
    "\n",
    "bar_y = cy + 40 \n",
    "cv2.rectangle(frame, (cx-50, bar_y), (cx-50+100, bar_y+10), (50, 50, 50), -1)\n",
    "cv2.rectangle(frame, (cx-50, bar_y), (cx-50+bar_width, bar_y+10), bar_color, -1)\n",
    "\n",
    "box_y_start = bar_y + 15\n",
    "box_y_end = box_y_start + 50\n",
    "cv2.rectangle(frame, (cx-50, box_y_start), (cx+50, box_y_end), (0, 0, 0), -1)\n",
    "frame = draw_arabic_text(frame, current_char, (cx-10, box_y_start + 5), (255, 255, 0))\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Computes `(cx, cy)` from the wrist landmark (reference point).\n",
    "\n",
    "- Draws a progress bar showing how close the stability check is to completing.\n",
    "\n",
    "- Draws a black box showing the current detected character.\n",
    "\n",
    "- Uses `draw_arabic_text` to display Arabic properly.\n",
    "\n",
    "**If removed**:\n",
    "The user would not see progress or character feedback on the hand â†’ a much worse interactive experience.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## 10- SPACE Cooldown\n",
    "```py\n",
    "if space_cooldown > 0: space_cooldown -= 1\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Decrements the cooldown counter every frame until it reaches zero.\n",
    "This prevents adding multiple spaces rapidly in consecutive frames.\n",
    "\n",
    "**If removed**:\n",
    "When the SPACE gesture appears once, multiple spaces could be added quickly as long as two hands stay detected.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## 11- Building a Wide Display Canvas\n",
    "```py\n",
    "final_ui_width = max(w_cam, MIN_WINDOW_WIDTH)\n",
    "total_height = UI_TOP_HEIGHT + h_cam + UI_BOTTOM_HEIGHT\n",
    "canvas = np.zeros((total_height, final_ui_width, 3), dtype=np.uint8)\n",
    "canvas[:] = (200, 80, 50)\n",
    "\n",
    "x_offset = (final_ui_width - w_cam) // 2\n",
    "canvas[UI_TOP_HEIGHT : UI_TOP_HEIGHT + h_cam, x_offset : x_offset + w_cam] = frame\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Creates a display canvas wider than the camera frame (to include a top bar, bottom bar, and text).\n",
    "\n",
    "- `canvas[:] = (200, 80, 50)` fills the background with a solid color.\n",
    "\n",
    "- Centers the camera frame inside the canvas using `x_offset`.\n",
    "\n",
    "**If removed**:\n",
    "Displaying only `frame` would remove the top UI bar, the bottom text area, and all layout formatting.\n",
    "\n",
    "---\n",
    "**Drawing Mode Status & Instructions**\n",
    "```py\n",
    "if writing_enabled:\n",
    "    mode_text = \"MODE: WRITING (ON)\"\n",
    "    mode_color = (0, 225, 0)\n",
    "else:\n",
    "    mode_text = \"MODE: PREVIEW (OFF)\"\n",
    "    mode_color = (0, 100, 255)\n",
    "\n",
    "cv2.putText(canvas, mode_text, (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, mode_color, 2)\n",
    "instructions = \"S:Write | C:Clear | Back:Undo | Q:Quit\"\n",
    "(text_w, _), _ = cv2.getTextSize(instructions, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "cv2.putText(canvas, instructions, (final_ui_width - text_w - 20, 35), cv2.FONT_HERSHEY_SIMPLEX, .8, (200, 250, 250), 1)\n",
    "cv2.line(canvas, (0, UI_TOP_HEIGHT), (final_ui_width, UI_TOP_HEIGHT), (50, 50, 50), 2)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Shows the mode status (writing or preview) and keyboard instructions in the top UI bar, then draws a separator line.\n",
    "\n",
    "**If removed**:\n",
    "The user would not know the current writing mode or the available shortcuts.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom Bar & Displaying the Translated Text**\n",
    "```py\n",
    "bottom_bar_y_start = UI_TOP_HEIGHT + h_cam\n",
    "cv2.line(canvas, (0, bottom_bar_y_start), (final_ui_width, bottom_bar_y_start), (50, 50, 50), 2)\n",
    "\n",
    "text_y_pos = bottom_bar_y_start + 15\n",
    "canvas = draw_arabic_text(canvas, \"Ø§Ù„Ù†Øµ: \" + sentence, (20, text_y_pos), (255, 255, 255), font_size=40)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Draws a horizontal line above the bottom bar, then displays the constructed text using `draw_arabic_text` (to support proper Arabic rendering).\n",
    "\n",
    "**If removed**:\n",
    "The final translated/generated sentence would not appear at the bottom of the screen.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## 12- Display, Keyboard Controls, and Exit Logic\n",
    "```py\n",
    "cv2.imshow('Tech mind - Sign Language Translator', canvas)\n",
    "key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "if key == ord('q'): break\n",
    "elif key == 8: sentence = sentence[:-1]\n",
    "elif key == ord('s'): writing_enabled = not writing_enabled\n",
    "elif key == ord('c'): sentence = \"\"\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Displays the window with a custom title.\n",
    "\n",
    "- `waitKey(1)` reads keyboard input.\n",
    "\n",
    "**Keyboard controls**:\n",
    "\n",
    "| Key | Function |\n",
    "| :---: | :--- |\n",
    "| **S** | Enable/Disable writing mode (Start/Stop) |\n",
    "| **C** | Clear the entire sentence |\n",
    "| **Back/8** | Delete the last character |\n",
    "| **Q** | Quit the program |\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Using key code 8 for Backspace depends on OpenCV + OS; on some systems you may need `127` or `ord('\\b')`.\n",
    "\n",
    "- Without `waitKey`, the program wouldn't update the window or read keyboard input.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## 13- Cleaning Up Resources\n",
    "```py\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Releases the camera and closes all OpenCV windows.\n",
    "\n",
    "**If removed**:\n",
    "The camera may remain locked, making it unusable for other applications; windows may stay open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6222bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 4. Ø§Ù„ØªØ´ØºÙŠÙ„\n",
    "# =============================================================\n",
    "cap = cv2.VideoCapture(myCamere) \n",
    "# Ù†Ø­Ø§ÙˆÙ„ Ø·Ù„Ø¨ Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ Ù„ÙƒÙ† Ø§Ù„ÙƒÙˆØ¯ Ø³ÙŠØªÙƒÙŠÙ Ù…Ù‡Ù…Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    # Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„Ø£ØµÙ„ÙŠ (Ù…Ù† Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§)\n",
    "    h_cam, w_cam, _ = frame.shape\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    # --- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙŠØ¯ÙŠÙ† (ÙƒÙ„ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù‡Ù†Ø§ Ø®Ø§ØµØ© Ø¨Ø¥Ø·Ø§Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙÙ‚Ø·) ---\n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_count = len(results.multi_hand_landmarks)\n",
    "\n",
    "        if hands_count == 2:\n",
    "            frame_counter = 0\n",
    "            if writing_enabled and space_cooldown == 0:\n",
    "                sentence += \" \"\n",
    "                space_cooldown = SPACE_COOLDOWN_MAX\n",
    "            \n",
    "            # Ø±Ø³Ù… ÙƒÙ„Ù…Ø© SPACE\n",
    "            cv2.putText(frame, \"SPACE\", (50, h_cam - 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        elif hands_count == 1:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            data = extract_features(hand_landmarks)\n",
    "            raw_char = \"?\"\n",
    "            try:\n",
    "                prediction = model.predict([data])[0]\n",
    "                raw_char = str(prediction)\n",
    "            except: pass\n",
    "\n",
    "            current_char = normalize_char(raw_char)\n",
    "\n",
    "            if current_char == last_prediction:\n",
    "                frame_counter += 1\n",
    "            else:\n",
    "                frame_counter = 0\n",
    "                last_prediction = current_char\n",
    "\n",
    "            if frame_counter >= THRESHOLD_FRAMES:\n",
    "                if writing_enabled:\n",
    "                    sentence += current_char\n",
    "                    frame_counter = 0 \n",
    "                else:\n",
    "                    frame_counter = THRESHOLD_FRAMES \n",
    "\n",
    "            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙƒØ§Ù† Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø¥Ø·Ø§Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§\n",
    "            cx, cy = int(hand_landmarks.landmark[0].x * w_cam), int(hand_landmarks.landmark[0].y * h_cam)\n",
    "            \n",
    "            # Ø§Ù„Ø±Ø³Ù… Ø¹Ù„Ù‰ Ø¥Ø·Ø§Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§\n",
    "            bar_width = int((frame_counter / THRESHOLD_FRAMES) * 100)\n",
    "            bar_color = (0, 255, 0) if writing_enabled else (0, 200, 255)\n",
    "            \n",
    "            bar_y = cy + 40 \n",
    "            cv2.rectangle(frame, (cx-50, bar_y), (cx-50+100, bar_y+10), (50, 50, 50), -1)\n",
    "            cv2.rectangle(frame, (cx-50, bar_y), (cx-50+bar_width, bar_y+10), bar_color, -1)\n",
    "            \n",
    "            box_y_start = bar_y + 15\n",
    "            box_y_end = box_y_start + 50\n",
    "            cv2.rectangle(frame, (cx-50, box_y_start), (cx+50, box_y_end), (0, 0, 0), -1)\n",
    "            frame = draw_arabic_text(frame, current_char, (cx-10, box_y_start + 5), (255, 255, 0))\n",
    "\n",
    "    if space_cooldown > 0: space_cooldown -= 1\n",
    "\n",
    "    # =========================================================\n",
    "    # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø±ÙŠØ¶Ø© (Canvas)\n",
    "    # =========================================================\n",
    "    \n",
    "    # 1. ØªØ­Ø¯ÙŠØ¯ Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø¥Ù…Ø§ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø«Ø§Ø¨Øª 1280 Ø£Ùˆ Ø¹Ø±Ø¶ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ Ø£ÙŠÙ‡Ù…Ø§ Ø£ÙƒØ¨Ø±)\n",
    "    final_ui_width = max(w_cam, MIN_WINDOW_WIDTH)\n",
    "    \n",
    "    # 2. Ø¥Ù†Ø´Ø§Ø¡ Ù„ÙˆØ­Ø© Ø³ÙˆØ¯Ø§Ø¡\n",
    "    total_height = UI_TOP_HEIGHT + h_cam + UI_BOTTOM_HEIGHT\n",
    "    canvas = np.zeros((total_height, final_ui_width, 3), dtype=np.uint8)\n",
    "    canvas[:] = (200, 80, 50)\n",
    "    \n",
    "    # 3. Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø·Ø© Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù„ØªÙˆØ³ÙŠØ· Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§\n",
    "    x_offset = (final_ui_width - w_cam) // 2\n",
    "    \n",
    "    # 4. ÙˆØ¶Ø¹ Ø¥Ø·Ø§Ø± Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙÙŠ Ù…Ù†ØªØµÙ Ø§Ù„Ù€ Canvas\n",
    "    canvas[UI_TOP_HEIGHT : UI_TOP_HEIGHT + h_cam, x_offset : x_offset + w_cam] = frame\n",
    "\n",
    "    # --- Ø±Ø³Ù… Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ø£Ø²Ø±Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù€ Canvas Ø§Ù„Ø¹Ø±ÙŠØ¶ ---\n",
    "    if writing_enabled:\n",
    "        mode_text = \"MODE: WRITING (ON)\"\n",
    "        mode_color = (0, 225, 0) \n",
    "    else:\n",
    "        mode_text = \"MODE: PREVIEW (OFF)\"\n",
    "        mode_color = (0, 100, 255)\n",
    "\n",
    "    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¹Ù„ÙˆÙŠ\n",
    "    cv2.putText(canvas, mode_text, (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, mode_color, 2)\n",
    "    \n",
    "    instructions = \"S:Write | C:Clear | Back:Undo | Q:Quit\"\n",
    "    (text_w, _), _ = cv2.getTextSize(instructions, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "    # Ù†Ø³ØªØ®Ø¯Ù… final_ui_width Ù„Ù„ØªØ£ÙƒØ¯ Ø§Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙÙŠ Ø§Ù‚ØµÙ‰ Ø§Ù„ÙŠÙ…ÙŠÙ† Ù„Ù„Ø´Ø§Ø´Ø© Ø§Ù„Ø¹Ø±ÙŠØ¶Ø©\n",
    "    cv2.putText(canvas, instructions, (final_ui_width - text_w - 20, 35), cv2.FONT_HERSHEY_SIMPLEX, .8, (200, 250, 250), 1)\n",
    "\n",
    "    cv2.line(canvas, (0, UI_TOP_HEIGHT), (final_ui_width, UI_TOP_HEIGHT), (50, 50, 50), 2)\n",
    "\n",
    "    # Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø³ÙÙ„ÙŠ\n",
    "    bottom_bar_y_start = UI_TOP_HEIGHT + h_cam\n",
    "    cv2.line(canvas, (0, bottom_bar_y_start), (final_ui_width, bottom_bar_y_start), (50, 50, 50), 2)\n",
    "    \n",
    "    # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ±Ø¬Ù…\n",
    "    text_y_pos = bottom_bar_y_start + 15\n",
    "    # Ø§Ù„Ù†Øµ Ø§Ù„Ø¢Ù† ÙŠÙ…Ù„Ùƒ Ù…Ø³Ø§Ø­Ø© 1280 Ø¨ÙŠÙƒØ³Ù„ ÙƒØ§Ù…Ù„Ø©\n",
    "    canvas = draw_arabic_text(canvas, \"Ø§Ù„Ù†Øµ: \" + sentence, (20, text_y_pos), (255, 255, 255), font_size=40)\n",
    "\n",
    "    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
    "    cv2.imshow('Tech mind - Sign Language Translator', canvas)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'): break\n",
    "    elif key == 8: sentence = sentence[:-1] \n",
    "    elif key == ord('s'): \n",
    "        writing_enabled = not writing_enabled\n",
    "    elif key == ord('c'): \n",
    "        sentence = \"\"\n",
    "        print(\">> Cleared All Text\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
