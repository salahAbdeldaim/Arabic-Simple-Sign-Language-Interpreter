import cv2
import mediapipe as mp
import numpy as np
import joblib
import os
from PIL import ImageFont, ImageDraw, Image
import arabic_reshaper
from bidi.algorithm import get_display

# =============================================================
# ğŸ›ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ÙˆØ§Ù„Ù…Ø³Ø§Ø±Ø§Øª
# =============================================================
# Ù‚Ù…Ù†Ø§ Ø¨ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ÙÙŠ Ù‚Ø§Ù…ÙˆØ³ Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØ¨Ø¯ÙŠÙ„
MODELS_CONFIG = {
    '1': {
        'name': 'KNN (Jana)',
        'path': r"models/knn_model.pkl"
    },
    '2': {
        'name': 'Linear SVC (Abdelaziz)',
        'path': r"models/LinearSVC_model.pkl"
    },
    '3': {
        'name': 'Logistic_Reg (Tarek)',
        'path': r"models/Logistic_Reg.pkl"
    },
    '4': {
        'name': 'Random Forest (Mohamed)',
        'path': r"models/RainForcement_Model.p"
    }
}

# =============================================================
# ğŸ› ï¸ Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© (ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ + Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª)
# =============================================================

def safe_load_model(path):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø³ÙˆØ§Ø¡ ÙƒØ§Ù† ÙƒØ§Ø¦Ù†Ø§Ù‹ Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ø£Ùˆ Ø¯Ø§Ø®Ù„ Ù‚Ø§Ù…ÙˆØ³"""
    if not os.path.exists(path):
        print(f"âŒ Ø®Ø·Ø£: Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {path}")
        return None
    
    try:
        loaded_obj = joblib.load(path)
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù‚Ø§Ù…ÙˆØ³Ø§Ù‹ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        if isinstance(loaded_obj, dict):
            if 'model' in loaded_obj:
                return loaded_obj['model']
            return loaded_obj # Ø±Ø¨Ù…Ø§ Ù‡Ùˆ Ù‚Ø§Ù…ÙˆØ³ ÙˆÙ„ÙƒÙ† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù‡Ùˆ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù†ÙØ³Ù‡ (Ø­Ø§Ù„Ø© Ù†Ø§Ø¯Ø±Ø©)
        return loaded_obj
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {path}: {e}")
        return None

def extract_features(hand_landmarks):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ 60 Ù…ÙŠØ²Ø© (Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ù†Ø³Ø¨ÙŠØ© Ø¨Ø¯ÙˆÙ† Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø¹ØµÙ…)"""
    points = []
    for lm in hand_landmarks.landmark:
        points.append([lm.x, lm.y, lm.z])

    # Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© (Ø§Ù„Ù…Ø¹ØµÙ…)
    base_x, base_y, base_z = points[0]
    final_features = []
    
    # Ù†Ø¨Ø¯Ø£ Ù…Ù† 1 (Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¹ØµÙ…) ÙˆÙ†Ø·Ø±Ø­ Ù‚ÙŠÙ…ØªÙ‡ Ù…Ù† Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù†Ù‚Ø·
    for i in range(1, len(points)):
        p = points[i]
        final_features.extend([p[0] - base_x, p[1] - base_y, p[2] - base_z])

    return final_features

def put_arabic_text(img, text, position, color=(0, 255, 0)):
    """Ø±Ø³Ù… Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø©"""
    img_pil = Image.fromarray(img)
    reshaped_text = arabic_reshaper.reshape(text)
    bidi_text = get_display(reshaped_text)
    draw = ImageDraw.Draw(img_pil)
    try:
        font = ImageFont.truetype("arial.ttf", 32)
    except:
        font = ImageFont.load_default()
    draw.text(position, bidi_text, font=font, fill=color)
    return np.array(img_pil)

# =============================================================
# ğŸš€ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
# =============================================================
print("ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª...")
loaded_models = {}
for key, config in MODELS_CONFIG.items():
    print(f"   ... ØªØ­Ù…ÙŠÙ„ {config['name']}")
    model = safe_load_model(config['path'])
    if model:
        loaded_models[key] = model
    else:
        print(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ {config['name']}")

if not loaded_models:
    print("âŒ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª.")
    exit()

# ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ (Ø±Ù‚Ù… 4 - Random Forest Ù„Ø£Ù†Ù‡ Ø§Ù„Ø£Ù‚ÙˆÙ‰)
current_key = '4'
current_model = loaded_models.get(current_key)
print(f"âœ… ØªÙ… Ø§Ù„Ø¬Ø§Ù‡Ø²ÙŠØ©! Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ: {MODELS_CONFIG[current_key]['name']}")
print("ğŸ’¡ Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… 1, 2, 3, 4 ÙÙŠ Ø§Ù„ÙƒÙŠØ¨ÙˆØ±Ø¯ Ù„Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„.")

# =============================================================
# ğŸ¥ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Mediapipe ÙˆØ§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
# =============================================================
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    min_detection_confidence=0.7,
    min_tracking_confidence=0.7
)

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break

    # Ù‚Ù„Ø¨ Ø§Ù„ØµÙˆØ±Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    # frame = cv2.flip(frame, 1)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_frame)
    
    # Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø±Ø¶
    prediction_text = "..."
    conf_text = ""
    color = (200, 200, 200)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            data = extract_features(hand_landmarks)

            # 2. Ø§Ù„ØªÙˆÙ‚Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ
            if current_model:
                try:
                    prediction = current_model.predict([data])[0]
                    prediction_text = str(prediction)
                    
                    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© (Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù…Ø«Ù„ SVC Ù„Ø§ ØªØ¯Ø¹Ù…Ù‡Ø§ Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹)
                    if hasattr(current_model, "predict_proba"):
                        probs = current_model.predict_proba([data])[0]
                        confidence = np.max(probs) * 100
                        conf_text = f"({int(confidence)}%)"
                        
                        if confidence < 60:
                            color = (0, 0, 255) # Ø£Ø­Ù…Ø± Ù„Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø©
                        else:
                            color = (0, 255, 0) # Ø£Ø®Ø¶Ø±
                    else:
                        # ÙÙŠ Ø­Ø§Ù„Ø© SVC Ø§Ùˆ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù„Ø§ ØªØ¯Ø¹Ù… Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
                        conf_text = "(N/A)" 
                        color = (255, 255, 0) # Ø£ØµÙØ±

                except Exception as e:
                    prediction_text = "Error"
                    print(f"Predict Error: {e}")

            # Ø±Ø³Ù… Ø§Ù„Ù†ØªÙŠØ¬Ø©
            h, w, c = frame.shape
            cx, cy = int(hand_landmarks.landmark[0].x * w), int(hand_landmarks.landmark[0].y * h)
            
            # Ø®Ù„ÙÙŠØ© Ø³ÙˆØ¯Ø§Ø¡ Ù„Ù„Ù†Øµ
            cv2.rectangle(frame, (cx-60, cy-90), (cx+160, cy-30), (0, 0, 0), -1)
            display_str = f"{prediction_text} {conf_text}"
            frame = put_arabic_text(frame, display_str, (cx-50, cy-85), color)

    # =========================================================
    # ğŸ–¥ï¸ ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø¹Ø±Ø¶ (UI)
    # =========================================================
    # Ø±Ø³Ù… Ø´Ø±ÙŠØ· Ø§Ù„Ø­Ø§Ù„Ø© ÙÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰
    cv2.rectangle(frame, (0, 0), (w, 40), (0, 0, 0), -1)
    model_name = MODELS_CONFIG[current_key]['name']
    ui_text = f"Current Model: {model_name} (Press 1-4 to switch)"
    cv2.putText(frame, ui_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

    cv2.imshow('Sign Language Benchmark', frame)

    # =========================================================
    # âŒ¨ï¸ Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„ÙƒÙŠØ¨ÙˆØ±Ø¯
    # =========================================================
    key = cv2.waitKey(1) & 0xFF
    if key == ord('q'):
        break
    elif key in [ord('1'), ord('2'), ord('3'), ord('4')]:
        new_key = chr(key)
        if new_key in loaded_models:
            current_key = new_key
            current_model = loaded_models[current_key]
            print(f"ğŸ”€ Switched to: {MODELS_CONFIG[current_key]['name']}")

cap.release()
cv2.destroyAllWindows()