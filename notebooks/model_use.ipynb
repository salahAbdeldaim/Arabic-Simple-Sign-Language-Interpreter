{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a36240e",
   "metadata": {},
   "source": [
    "## **1- Imports and Warning Suppression Setup**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dd05a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d32b43",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "Imports the warnings module and disables Python warnings so that no warning messages appear in the output during execution.\n",
    "\n",
    "**If removed**:\n",
    "You may see warnings from libraries (like deprecation warnings) that could clutter the terminal, but the code will usually still run normally.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b1a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                                    #==> used for video processing\n",
    "import mediapipe as mp                        #==> for hand tracking (landmarks)\n",
    "import numpy as np                            #==> handling images as arrays\n",
    "import joblib                                 #==> lood the saved model \n",
    "import arabic_reshaper                        #==> Used to draw Arabic text\n",
    "from bidi.algorithm import get_display        #==> Used to draw Arabic text\n",
    "from PIL import ImageFont, ImageDraw, Image   #==> Used to draw Arabic text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46754603",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation for each library**:\n",
    "\n",
    "- `cv2`: OpenCV, used for video processing, drawing on frames, and displaying windows.\n",
    "\n",
    "- `mediapipe`: A ready-made library for hand tracking (landmarks).\n",
    "\n",
    "- `numpy`: For array operations and handling images as arrays.\n",
    "\n",
    "- `joblib`: For loading the saved model (pickled model).\n",
    "\n",
    "- `arabic_reshaper + bidi.algorithm.get_display`: Reconstructs Arabic characters and fixes direction for correct display (used to render Arabic text with PIL/OpenCV).\n",
    "\n",
    "- `PIL` (ImageFont, ImageDraw, Image): Used to draw Arabic text on images because OpenCV does not support Arabic rendering properly.\n",
    "\n",
    "**If any of these are removed**:\n",
    "The part of the code depending on that library will fail (e.g., without joblib you can't load the model; without arabic_reshaper, Arabic text will appear disjointed).\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad1027",
   "metadata": {},
   "source": [
    "## **2- General Settings / Constant Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ce3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Settings...\n",
    "MODEL_PATH = r\"linearSVC.pkl\" #==> The file path\n",
    "THRESHOLD_FRAMES = 25         #==> Frames required to confirm a predicted letter\n",
    "SPACE_COOLDOWN_MAX = 40       #==> Cooldown frames after adding a space\n",
    "myCamere = 0                  #==> Camera index\n",
    "\n",
    "# UI settings\n",
    "UI_TOP_HEIGHT = 50      \n",
    "UI_BOTTOM_HEIGHT = 80  \n",
    "MIN_WINDOW_WIDTH = 800 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205b693",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- `MODEL_PATH`: Path to the saved model file (pickle).\n",
    "\n",
    "- `THRESHOLD_FRAMES`: Number of frames required to confirm a predicted letter before adding it — prevents capturing multiple times from a single frame.\n",
    "\n",
    "- `SPACE_COOLDOWN_MAX`: Number of cooldown frames after adding a space (SPACE) before allowing another one.\n",
    "\n",
    "- `myCamere`: Camera index (0 is usually the default webcam).\n",
    "\n",
    "**UI settings**:\n",
    "Top/bottom bar height and minimum window width (to avoid overlap in UI elements).\n",
    "\n",
    "**If changed/removed**:\n",
    "\n",
    "- Lowering `THRESHOLD_FRAMES` → faster but more prone to errors.\n",
    "\n",
    "- Increasing it → slower input but more stable.\n",
    "\n",
    "- Setting `MIN_WINDOW_WIDTH` too small → text may overlap or collide with UI elements.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc7fca",
   "metadata": {},
   "source": [
    "## **3- State Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557d8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Variables\n",
    "sentence = \"\"            #==> The actual constructed text\n",
    "last_prediction = None   #==> The last predicted letter\n",
    "frame_counter = 0        #==> Counts how many consecutive frames kept the same prediction\n",
    "space_cooldown = 0       #==> Cooldown counter for adding a SPACE\n",
    "writing_enabled = False  #==> Whether writing mode is enabled or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ad68a",
   "metadata": {},
   "source": [
    "**Explanation**: Variables that store the current state:\n",
    "\n",
    "- `sentence`: The actual constructed text.\n",
    "\n",
    "- `last_prediction`: The last predicted letter — used to check stability across frames.\n",
    "\n",
    "- `frame_counter`: Counts how many consecutive frames kept the same prediction.\n",
    "\n",
    "- `space_cooldown`: Cooldown counter for adding a SPACE.\n",
    "\n",
    "- `writing_enabled`: Whether writing mode is enabled or not.\n",
    "\n",
    "**If removed**:\n",
    "Text tracking and input stabilization would break or behave incorrectly.\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cab16",
   "metadata": {},
   "source": [
    "## **4- Loading the Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6ba17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Model...\")\n",
    "\n",
    "try:\n",
    "    # Try to load the model file using joblib\n",
    "    loaded_data = joblib.load(MODEL_PATH)\n",
    "\n",
    "    # Check if the loaded object is a dictionary containing a 'model' key\n",
    "    if isinstance(loaded_data, dict) and 'model' in loaded_data:\n",
    "        model = loaded_data['model']  #==> Extract the model from the dictionary\n",
    "    else:\n",
    "        model = loaded_data  #==> Otherwise, use the loaded object directly\n",
    "\n",
    "    print(\"Model Loaded.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # If loading the model fails, print an error message\n",
    "    print(\"Error loading model.\")\n",
    "    print(e)\n",
    "    exit()  #==> Stop the program if the model fails to load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cde5eb4",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- Attempts to load the file `linearSVC.pkl`.\n",
    "\n",
    "- Some people store models in a dictionary like `{'model': model, ...}`, so this code handles both possibilities.\n",
    "\n",
    "- If loading fails, it prints an error and exits the program (`exit()`).\n",
    "\n",
    "**If `try/except` is removed**:\n",
    "A loading error would raise an exception and crash the program in a less friendly way.\n",
    "\n",
    "**Note**:\n",
    "During development, you may prefer printing the actual error cause:\n",
    "`except Exception as e: print(e)`\n",
    "\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b577b",
   "metadata": {},
   "source": [
    "## **5- Helper Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119dd8dc",
   "metadata": {},
   "source": [
    "**`normalize_char`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fee2cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char(char):\n",
    "    \"\"\"\n",
    "    Normalize Arabic characters.\n",
    "    \n",
    "    Args:\n",
    "        char (str): A single character to normalize.\n",
    "    \n",
    "    Returns:\n",
    "        str: The normalized character ('أ' becomes 'ا').\n",
    "    \"\"\"\n",
    "    if char == 'أ':\n",
    "        return 'ا'\n",
    "    return char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2174e67",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "Normalizes certain Arabic letters into a unified form `(here it converts \"أ\" to \"ا\")`.\n",
    "\n",
    "**If removed**:\n",
    "Visual variations may appear (e.g., `\"أ\"` and `\"ا\"` would be treated as different characters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26221a21",
   "metadata": {},
   "source": [
    "**`extract_features`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49869fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Extract normalized features from hand landmarks.\n",
    "\n",
    "    This function converts absolute landmark coordinates into\n",
    "    relative coordinates by subtracting the base point (landmark 0).\n",
    "    \n",
    "    Args:\n",
    "        hand_landmarks: Mediapipe hand landmarks object.\n",
    "    \n",
    "    Returns:\n",
    "        list: A flat list of normalized (x, y, z) features.\n",
    "    \"\"\"\n",
    "    \n",
    "    points = []\n",
    "    \n",
    "    # Collect all landmark coordinates (x, y, z)\n",
    "    for lm in hand_landmarks.landmark:\n",
    "        points.append([lm.x, lm.y, lm.z])\n",
    "\n",
    "    # Use landmark 0 as the reference point (base)\n",
    "    base_x, base_y, base_z = points[0]\n",
    "\n",
    "    final_features = []\n",
    "\n",
    "    # Normalize each point relative to the base point\n",
    "    for i in range(1, len(points)):\n",
    "        p = points[i]\n",
    "        final_features.extend([\n",
    "            p[0] - base_x,\n",
    "            p[1] - base_y,\n",
    "            p[2] - base_z\n",
    "        ])\n",
    "\n",
    "    return final_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c737176",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "Converts Mediapipe `hand_landmarks` coordinates into a feature list relative to a base point (index 0).\n",
    "\n",
    "- Assumes `points[0]` is the reference point (usually the wrist).\n",
    "\n",
    "- Returns coordinate differences for each landmark — producing a representation independent of camera distance/position.\n",
    "\n",
    "**If removed/changed**:\n",
    "The model expects features in a specific format.\n",
    "Changing the extraction method will break compatibility with the trained model.\n",
    "Using raw coordinates without normalization would make the model sensitive to distance and hand position.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2759c",
   "metadata": {},
   "source": [
    "**`draw_arabic_text`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389a95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_arabic_text(img, text, position, color=(0, 255, 0), font_size=32):\n",
    "    \"\"\"\n",
    "    Draw Arabic text correctly (reshaped + RTL) on an OpenCV image.\n",
    "\n",
    "    This function fixes Arabic text rendering issues by reshaping the letters\n",
    "    and applying right-to-left ordering before drawing using Pillow.\n",
    "\n",
    "    Args:\n",
    "        img (numpy.ndarray): The image (OpenCV format BGR) to draw on.\n",
    "        text (str): The Arabic text you want to display.\n",
    "        position (tuple): (x, y) coordinates where the text will appear.\n",
    "        color (tuple): Text color in RGB format (default: green).\n",
    "        font_size (int): Font size for the text.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The updated image with Arabic text drawn on it.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert image from OpenCV (numpy) to PIL Image\n",
    "    img_pil = Image.fromarray(img)\n",
    "\n",
    "    # Reshape Arabic letters so they connect properly\n",
    "    reshaped_text = arabic_reshaper.reshape(text)\n",
    "    \n",
    "    # Apply right-to-left ordering\n",
    "    bidi_text = get_display(reshaped_text)\n",
    "\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    # Load Arabic-compatible font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Ensure coordinates are integers\n",
    "    position = (int(position[0]), int(position[1]))\n",
    "\n",
    "    # Draw the processed Arabic text\n",
    "    draw.text(position, bidi_text, font=font, fill=color)\n",
    "\n",
    "    # Convert PIL image back to OpenCV (numpy)\n",
    "    return np.array(img_pil)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccad3fb",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "Draws Arabic text on an image (numpy array) using PIL:\n",
    "\n",
    "- Converts the numpy image to a PIL image.\n",
    "\n",
    "- Uses `arabic_reshaper` + `bidi.get_display` to render Arabic letters correctly.\n",
    "\n",
    "Draws the text using the chosen font, falling back to default if `arial.ttf` is missing.\n",
    "\n",
    "- Returns the modified image as a numpy array.\n",
    "\n",
    "**If removed**:\n",
    "Displaying Arabic text using `cv2.putText` results in broken or reversed letters.\n",
    "Arabic shaping and direction handling will not work.\n",
    "This function is essential for proper Arabic rendering.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d8d7b",
   "metadata": {},
   "source": [
    "## **6- Mediapipe Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e97b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Mediapipe modules\n",
    "mp_hands = mp.solutions.hands            #==> Hand landmarks detection module\n",
    "mp_drawing = mp.solutions.drawing_utils  #==> Utilities for drawing landmarks\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "        static_image_mode=False,         #==> Video mode (faster tracking)\n",
    "        max_num_hands=2,                 #==> Detect up to 2 hands\n",
    "        min_detection_confidence=0.7,    #==> Confidence threshold for detection\n",
    "        min_tracking_confidence=0.7      #==> Confidence threshold for tracking\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd968d0",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "\n",
    "- `mp_hands` and `mp_drawing` provide hand-tracking and drawing utilities.\n",
    "\n",
    "- `Hands` is configured for:\n",
    "\n",
    "- Video input (not static images),\n",
    "\n",
    "- Up to 2 hands,\n",
    "\n",
    "- Confidence thresholds for detection and tracking.\n",
    "\n",
    "**If values are changed**:\n",
    "\n",
    "- Lower `min_detection_confidence` → more false positives.\n",
    "\n",
    "- Higher values → may fail to detect hands in poor lighting.\n",
    "\n",
    "- `max_num_hands` > 2 is unnecessary in most cases.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e25c6",
   "metadata": {},
   "source": [
    "## **7- The Main loop (Pumping Life)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec326aba",
   "metadata": {},
   "source": [
    "**`Camera setup`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0352ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(myCamere) \n",
    "# Request high resolution (camera may return different actual size)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17bda3",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "Opens your device camera (using index `myCamere`) and attempts to set the resolution to 1280×720.\n",
    "\n",
    "**If `cap.set` is removed**:\n",
    "The camera will run at its default resolution (usually lower).\n",
    "Note: Some cameras cannot support the requested resolution and will fallback to what they support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3c52d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0cb20",
   "metadata": {},
   "source": [
    "**`MAIN LOOP (Camera + Processing)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6222bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  #==> Cannot read frame → stop\n",
    "\n",
    "    # Actual camera resolution (may differ from requested)\n",
    "    h_cam, w_cam, _ = frame.shape\n",
    "\n",
    "    # Mediapipe expects RGB images\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # HAND PROCESSING\n",
    "    # ---------------------------------------------------------\n",
    "    if results.multi_hand_landmarks:\n",
    "        hands_count = len(results.multi_hand_landmarks)\n",
    "\n",
    "        # ---------------------- TWO HANDS ----------------------\n",
    "        if hands_count == 2:\n",
    "            frame_counter = 0  # Reset counter → avoid unwanted characters\n",
    "\n",
    "            # Add a space only if writing is active and cooldown is zero\n",
    "            if writing_enabled and space_cooldown == 0:\n",
    "                sentence += \" \"\n",
    "                space_cooldown = SPACE_COOLDOWN_MAX\n",
    "\n",
    "            # Visual note: SPACE detected\n",
    "            cv2.putText(frame, \"SPACE\", (50, h_cam - 50), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        1, (255, 0, 0), 2)\n",
    "\n",
    "            # Draw both hands' landmarks\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # ---------------------- ONE HAND ----------------------\n",
    "        elif hands_count == 1:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Feature extraction (should return 63 values for 21 landmarks)\n",
    "            data = extract_features(hand_landmarks)\n",
    "\n",
    "            raw_char = \"?\"\n",
    "            try:\n",
    "                # Model expects a list of samples → wrap \"data\" inside [ ]\n",
    "                prediction = model.predict([data])[0]\n",
    "                raw_char = str(prediction)\n",
    "            except:\n",
    "                pass  #==> Avoid crashing if prediction fails\n",
    "\n",
    "            # Normalize Arabic variations (ex: \"أ\" → \"ا\")\n",
    "            current_char = normalize_char(raw_char)\n",
    "\n",
    "            # Stable-frame system (debounce)\n",
    "            if current_char == last_prediction:\n",
    "                frame_counter += 1\n",
    "            else:\n",
    "                frame_counter = 0\n",
    "                last_prediction = current_char\n",
    "\n",
    "            # If prediction is stable enough, accept the character\n",
    "            if frame_counter >= THRESHOLD_FRAMES:\n",
    "                if writing_enabled:\n",
    "                    sentence += current_char\n",
    "                    frame_counter = 0  #==> Reset after adding character\n",
    "                else:\n",
    "                    # Stay in preview mode without adding\n",
    "                    frame_counter = THRESHOLD_FRAMES\n",
    "\n",
    "            # ---------------------------------------------\n",
    "            # DRAWING THE DETECTION UI NEAR THE HAND\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            # Convert landmark[0] from relative → pixel coordinates\n",
    "            cx = int(hand_landmarks.landmark[0].x * w_cam)\n",
    "            cy = int(hand_landmarks.landmark[0].y * h_cam)\n",
    "\n",
    "            # Fill amount of the progress bar (0 → 100px)\n",
    "            bar_width = int((frame_counter / THRESHOLD_FRAMES) * 100)\n",
    "            bar_color = (0, 255, 0) if writing_enabled else (0, 200, 255)\n",
    "\n",
    "            # Draw progress bar background + filled portion\n",
    "            bar_y = cy + 40\n",
    "            cv2.rectangle(frame, (cx-50, bar_y), (cx+50, bar_y+10), (50, 50, 50), -1)\n",
    "            cv2.rectangle(frame, (cx-50, bar_y), (cx-50+bar_width, bar_y+10), bar_color, -1)\n",
    "\n",
    "            # Character box under progress bar\n",
    "            box_y_start = bar_y + 15\n",
    "            box_y_end = box_y_start + 50\n",
    "            cv2.rectangle(frame, (cx-50, box_y_start), (cx+50, box_y_end), (0, 0, 0), -1)\n",
    "\n",
    "            # Note: draw_arabic_text uses PIL (RGB)\n",
    "            frame = draw_arabic_text(frame, current_char, (cx-10, box_y_start + 5),\n",
    "                                     (255, 255, 0))\n",
    "\n",
    "    # Space cooldown (to prevent repeated spaces)\n",
    "    if space_cooldown > 0:\n",
    "        space_cooldown -= 1\n",
    "\n",
    "    # =========================================================\n",
    "    # BUILDING THE WIDE UI CANVAS\n",
    "    # =========================================================\n",
    "\n",
    "    # Ensure final width is not smaller than the minimum UI width\n",
    "    final_ui_width = max(w_cam, MIN_WINDOW_WIDTH)\n",
    "\n",
    "    # Full canvas height = top UI + camera frame + bottom UI\n",
    "    total_height = UI_TOP_HEIGHT + h_cam + UI_BOTTOM_HEIGHT\n",
    "\n",
    "    # Create the background canvas\n",
    "    canvas = np.zeros((total_height, final_ui_width, 3), dtype=np.uint8)\n",
    "    canvas[:] = (200, 80, 50)  #==> Background color (BGR)\n",
    "\n",
    "    # Center the camera horizontally inside the wide UI\n",
    "    x_offset = (final_ui_width - w_cam) // 2\n",
    "\n",
    "    # Place the camera frame in the center area\n",
    "    canvas[UI_TOP_HEIGHT : UI_TOP_HEIGHT + h_cam, x_offset : x_offset + w_cam] = frame\n",
    "\n",
    "    # ---------------------- TOP BAR ----------------------\n",
    "    if writing_enabled:\n",
    "        mode_text = \"MODE: WRITING (ON)\"\n",
    "        mode_color = (0, 225, 0)\n",
    "    else:\n",
    "        mode_text = \"MODE: PREVIEW (OFF)\"\n",
    "        mode_color = (0, 100, 255)\n",
    "\n",
    "    # Draw mode text\n",
    "    cv2.putText(canvas, mode_text, (20, 35), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.8, mode_color, 2)\n",
    "\n",
    "    # Draw keyboard instructions\n",
    "    instructions = \"S:Write | C:Clear | Back:Undo | Q:Quit\"\n",
    "    (text_w, _), _ = cv2.getTextSize(instructions, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "\n",
    "    # Draw instructions aligned to the right\n",
    "    cv2.putText(canvas, instructions, (final_ui_width - text_w - 20, 35),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (200, 250, 250), 1)\n",
    "\n",
    "    # Divider line under the top bar\n",
    "    cv2.line(canvas, (0, UI_TOP_HEIGHT), (final_ui_width, UI_TOP_HEIGHT), (50, 50, 50), 2)\n",
    "\n",
    "    # ---------------------- BOTTOM BAR ----------------------\n",
    "    bottom_bar_y_start = UI_TOP_HEIGHT + h_cam\n",
    "\n",
    "    cv2.line(canvas, (0, bottom_bar_y_start), (final_ui_width, bottom_bar_y_start),\n",
    "             (50, 50, 50), 2)\n",
    "\n",
    "    # Draw the full translated text (Arabic)\n",
    "    text_y_pos = bottom_bar_y_start + 15\n",
    "    canvas = draw_arabic_text(canvas, \"النص: \" + sentence,\n",
    "                              (20, text_y_pos), (255, 255, 255), font_size=40)\n",
    "\n",
    "    # Display final UI window\n",
    "    cv2.imshow('Tech mind - Sign Language Translator', canvas)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # KEYBOARD INPUT HANDLING\n",
    "    # ---------------------------------------\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break  #==> Quit program\n",
    "\n",
    "    elif key == 8:\n",
    "        sentence = sentence[:-1]  #==> Backspace/Undo\n",
    "\n",
    "    elif key == ord('s'):\n",
    "        writing_enabled = not writing_enabled  #==> Toggle writing mode\n",
    "\n",
    "    elif key == ord('c'):\n",
    "        sentence = \"\"  #==> Clear all text\n",
    "        print(\">> Cleared All Text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b61f16",
   "metadata": {},
   "source": [
    "**1- The Main Loop**\n",
    "```py\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "A loop that keeps capturing frames as long as the camera is open.\n",
    "\n",
    "If `ret` is `False`, this means a capture error or end of stream → break.\n",
    "\n",
    "```py\n",
    "h_cam, w_cam, _ = frame.shape\n",
    "rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(rgb_frame)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Retrieves the frame dimensions.\n",
    "\n",
    "- Converts BGR → RGB because Mediapipe expects RGB.\n",
    "\n",
    "- `hands.process` returns the detected hand landmarks.\n",
    "\n",
    "**If RGB conversion is forgotten**:\n",
    "Mediapipe may perform poorly or give incorrect landmark detection because it would receive incorrectly ordered color channels.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**2- Hand Processing (If results are found)**\n",
    "```py\n",
    "if results.multi_hand_landmarks:\n",
    "    hands_count = len(results.multi_hand_landmarks)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "If one or more hands are detected, process them depending on how many hands are found.\n",
    "\n",
    "---\n",
    "\n",
    "**Case: Two Hands Detected (`hands_count == 2`)**\n",
    "```py\n",
    "if hands_count == 2:\n",
    "    frame_counter = 0\n",
    "    if writing_enabled and space_cooldown == 0:\n",
    "        sentence += \" \"\n",
    "        space_cooldown = SPACE_COOLDOWN_MAX\n",
    "\n",
    "    cv2.putText(frame, \"SPACE\", (50, h_cam - 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "Two hands are treated as the gesture for *SPACE*. When this occurs:\n",
    "\n",
    "- Reset `frame_counter` (to avoid recording a letter at this moment).\n",
    "\n",
    "- If writing is enabled and there is no cooldown, add a space to `sentence` and start the cooldown timer.\n",
    "\n",
    "- Display `\"SPACE\"` on the frame.\n",
    "\n",
    "- Draw landmarks for both hands.\n",
    "\n",
    "**If removed**:\n",
    "The system will not detect the SPACE gesture (2-hand gesture), will not insert spaces, and will not display `\"SPACE\"`.\n",
    "\n",
    "---\n",
    "**Case: One Hand Detected (`hands_count == 1`)**\n",
    "```py\n",
    "elif hands_count == 1:\n",
    "    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    data = extract_features(hand_landmarks)\n",
    "    raw_char = \"?\"\n",
    "    try:\n",
    "        prediction = model.predict([data])[0]\n",
    "        raw_char = str(prediction)\n",
    "    except: pass\n",
    "\n",
    "    current_char = normalize_char(raw_char)\n",
    "```\n",
    "\n",
    "---\n",
    "**Step-by-step explanation:**\n",
    "\n",
    "- Obtain the single hand's landmarks and draw them.\n",
    "\n",
    "- Extract features using `extract_features`.\n",
    "\n",
    "- Feed the features to the model to predict the corresponding character.\n",
    "\n",
    "- If prediction fails (incorrect data shape, etc.), ignore and keep `\"?\"`.\n",
    "\n",
    "- Normalize Arabic characters using `normalize_char`.\n",
    "\n",
    "**If the try/except is removed**:\n",
    "If the model throws any error, the entire loop would crash.\n",
    "The `try` ensures the app keeps running even when a prediction occasionally fails.\n",
    "\n",
    "---\n",
    "\n",
    "**Frame Stability Logic (Debouncing)**\n",
    "```py\n",
    "if current_char == last_prediction:\n",
    "    frame_counter += 1\n",
    "else:\n",
    "    frame_counter = 0\n",
    "    last_prediction = current_char\n",
    "\n",
    "if frame_counter >= THRESHOLD_FRAMES:\n",
    "    if writing_enabled:\n",
    "        sentence += current_char\n",
    "        frame_counter = 0\n",
    "    else:\n",
    "        frame_counter = THRESHOLD_FRAMES\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Compares the current prediction with the last one.\n",
    "\n",
    "- If the same prediction persists for `THRESHOLD_FRAMES`, it is considered stable.\n",
    "\n",
    "- Only then is the character added to `sentence` (if writing is enabled).\n",
    "\n",
    "This avoids recording letters from noisy or unstable frames.\n",
    "\n",
    "**If removed**:\n",
    "The system would add a letter immediately on every prediction → extremely noisy, unstable, and full of repeated letters.\n",
    "\n",
    "---\n",
    "\n",
    "**Using Hand Position for Drawing (Progress Bar & Character Box)**\n",
    "```py\n",
    "cx, cy = int(hand_landmarks.landmark[0].x * w_cam), int(hand_landmarks.landmark[0].y * h_cam)\n",
    "bar_width = int((frame_counter / THRESHOLD_FRAMES) * 100)\n",
    "bar_color = (0, 255, 0) if writing_enabled else (0, 200, 255)\n",
    "\n",
    "bar_y = cy + 40 \n",
    "cv2.rectangle(frame, (cx-50, bar_y), (cx-50+100, bar_y+10), (50, 50, 50), -1)\n",
    "cv2.rectangle(frame, (cx-50, bar_y), (cx-50+bar_width, bar_y+10), bar_color, -1)\n",
    "\n",
    "box_y_start = bar_y + 15\n",
    "box_y_end = box_y_start + 50\n",
    "cv2.rectangle(frame, (cx-50, box_y_start), (cx+50, box_y_end), (0, 0, 0), -1)\n",
    "frame = draw_arabic_text(frame, current_char, (cx-10, box_y_start + 5), (255, 255, 0))\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Computes `(cx, cy)` from the wrist landmark (reference point).\n",
    "\n",
    "- Draws a progress bar showing how close the stability check is to completing.\n",
    "\n",
    "- Draws a black box showing the current detected character.\n",
    "\n",
    "- Uses `draw_arabic_text` to display Arabic properly.\n",
    "\n",
    "**If removed**:\n",
    "The user would not see progress or character feedback on the hand → a much worse interactive experience.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**3- SPACE Cooldown**\n",
    "```py\n",
    "if space_cooldown > 0: space_cooldown -= 1\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Decrements the cooldown counter every frame until it reaches zero.\n",
    "This prevents adding multiple spaces rapidly in consecutive frames.\n",
    "\n",
    "**If removed**:\n",
    "When the SPACE gesture appears once, multiple spaces could be added quickly as long as two hands stay detected.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**4- Building a Wide Display Canvas**\n",
    "```py\n",
    "final_ui_width = max(w_cam, MIN_WINDOW_WIDTH)\n",
    "total_height = UI_TOP_HEIGHT + h_cam + UI_BOTTOM_HEIGHT\n",
    "canvas = np.zeros((total_height, final_ui_width, 3), dtype=np.uint8)\n",
    "canvas[:] = (200, 80, 50)\n",
    "\n",
    "x_offset = (final_ui_width - w_cam) // 2\n",
    "canvas[UI_TOP_HEIGHT : UI_TOP_HEIGHT + h_cam, x_offset : x_offset + w_cam] = frame\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Creates a display canvas wider than the camera frame (to include a top bar, bottom bar, and text).\n",
    "\n",
    "- `canvas[:] = (200, 80, 50)` fills the background with a solid color.\n",
    "\n",
    "- Centers the camera frame inside the canvas using `x_offset`.\n",
    "\n",
    "**If removed**:\n",
    "Displaying only `frame` would remove the top UI bar, the bottom text area, and all layout formatting.\n",
    "\n",
    "---\n",
    "**Drawing Mode Status & Instructions**\n",
    "```py\n",
    "if writing_enabled:\n",
    "    mode_text = \"MODE: WRITING (ON)\"\n",
    "    mode_color = (0, 225, 0)\n",
    "else:\n",
    "    mode_text = \"MODE: PREVIEW (OFF)\"\n",
    "    mode_color = (0, 100, 255)\n",
    "\n",
    "cv2.putText(canvas, mode_text, (20, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.8, mode_color, 2)\n",
    "instructions = \"S:Write | C:Clear | Back:Undo | Q:Quit\"\n",
    "(text_w, _), _ = cv2.getTextSize(instructions, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "cv2.putText(canvas, instructions, (final_ui_width - text_w - 20, 35), cv2.FONT_HERSHEY_SIMPLEX, .8, (200, 250, 250), 1)\n",
    "cv2.line(canvas, (0, UI_TOP_HEIGHT), (final_ui_width, UI_TOP_HEIGHT), (50, 50, 50), 2)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Shows the mode status (writing or preview) and keyboard instructions in the top UI bar, then draws a separator line.\n",
    "\n",
    "**If removed**:\n",
    "The user would not know the current writing mode or the available shortcuts.\n",
    "\n",
    "---\n",
    "\n",
    "**Bottom Bar & Displaying the Translated Text**\n",
    "```py\n",
    "bottom_bar_y_start = UI_TOP_HEIGHT + h_cam\n",
    "cv2.line(canvas, (0, bottom_bar_y_start), (final_ui_width, bottom_bar_y_start), (50, 50, 50), 2)\n",
    "\n",
    "text_y_pos = bottom_bar_y_start + 15\n",
    "canvas = draw_arabic_text(canvas, \"النص: \" + sentence, (20, text_y_pos), (255, 255, 255), font_size=40)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "Draws a horizontal line above the bottom bar, then displays the constructed text using `draw_arabic_text` (to support proper Arabic rendering).\n",
    "\n",
    "**If removed**:\n",
    "The final translated/generated sentence would not appear at the bottom of the screen.\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**5- Display, Keyboard Controls, and Exit Logic**\n",
    "```py\n",
    "cv2.imshow('Tech mind - Sign Language Translator', canvas)\n",
    "key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "if key == ord('q'): break\n",
    "elif key == 8: sentence = sentence[:-1]\n",
    "elif key == ord('s'): writing_enabled = not writing_enabled\n",
    "elif key == ord('c'): sentence = \"\"\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Displays the window with a custom title.\n",
    "\n",
    "- `waitKey(1)` reads keyboard input.\n",
    "\n",
    "**Keyboard controls**:\n",
    "\n",
    "| Key | Function |\n",
    "| :---: | :--- |\n",
    "| **S** | Enable/Disable writing mode (Start/Stop) |\n",
    "| **C** | Clear the entire sentence |\n",
    "| **Back/8** | Delete the last character |\n",
    "| **Q** | Quit the program |\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Using key code 8 for Backspace depends on OpenCV + OS; on some systems you may need `127` or `ord('\\b')`.\n",
    "\n",
    "- Without `waitKey`, the program wouldn't update the window or read keyboard input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6825105",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85cf1aa",
   "metadata": {},
   "source": [
    "**`Cleanup (run after stopping)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eccf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee9255a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
